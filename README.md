# Bridging Modal Gaps in Multimodal Sentiment Analysis: A Self-Supervised Text-Guided Fusion Approach

<br/>

## Backgroud and Significance

●    With the rapid proliferation of multimodal content—such as text, audio, and video—on social media and short video platforms, multimodal sentiment analysis (MSA) has emerged as a vital research direction in the field of artificial intelligence. Despite notable advancements in multimodal dataset construction and fusion strategies, MSA continues to face two fundamental challenges: the effective fusion of heterogeneous modalities and collaborative learning under modality imbalance. Besides, the inherent distributional differences between modalities often lead to conflicting or unstable results during fusion, significantly hindering the accuracy of sentiment prediction.

## The Framework

●   The framework of SETGF. For more details, please wait for the review.

## Usage

### Prerequisites

●    Python ==3.8.18

●    Pytorch==1.13.0

●    CUDA ==12.7

### Datasets and pre-trained berts

Download dataset features and pre-trained berts from the following links.

●   Dataset [Google Cloud Drive](https://drive.google.com/drive/folders/1E5kojBirtd5VbfHsFp6FYWkQunk73Nsv?usp=sharing)

●   Pre-trained berts [BERT](https://github.com/google-research/bert)

For more details, please refer to  [Self-mm](https://github.com/thuiar/Self-MM)

### Run the codes

●   You can first set the training dataset name in run.py as "mosei" , "mosi" or "sims", and then run.

### Results

●   You can view the results in the logs folder.

## Notes

● Please wait for the review to end for more details。